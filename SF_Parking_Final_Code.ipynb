{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SF Parking Finder"
      ],
      "metadata": {
        "id": "Ua0ZN8jvYqDl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwBfYkOM5UHn"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, hour, dayofweek, lit, count, avg, rand, when\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from datetime import datetime, timedelta, date\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from IPython.display import display, IFrame, FileLink\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from geopy.distance import geodesic\n",
        "import gc\n",
        "\n",
        "# Initialize Spark session with increased memory for full dataset\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SmartParkingFinder\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set memory fraction for better resource management\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "\n",
        "def load_parking_meter_data():\n",
        "    \"\"\"Load ALL parking meter data without any limits or sampling.\"\"\"\n",
        "    try:\n",
        "        # Get all records by setting a very high limit or using pagination\n",
        "        all_data = []\n",
        "        offset = 0\n",
        "        batch_size = 50000  # Process in batches to handle large dataset\n",
        "\n",
        "        while True:\n",
        "            url = f\"https://data.sfgov.org/resource/8vzz-qzz9.json?$limit={batch_size}&$offset={offset}\"\n",
        "            response = requests.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            batch_data = response.json()\n",
        "\n",
        "            if not batch_data:  # No more data\n",
        "                break\n",
        "\n",
        "            all_data.extend(batch_data)\n",
        "            offset += batch_size\n",
        "            print(f\"üì• Loaded batch: {len(batch_data)} records (Total: {len(all_data)})\")\n",
        "\n",
        "            # Break if we get less than batch_size (indicates end of data)\n",
        "            if len(batch_data) < batch_size:\n",
        "                break\n",
        "\n",
        "        # Keep all essential columns - no filtering\n",
        "        df = spark.createDataFrame(pd.DataFrame(all_data)[['post_id', 'street_name', 'street_num', 'latitude', 'longitude', 'analysis_neighborhood']])\n",
        "        df = df.withColumn(\"latitude\", col(\"latitude\").cast(\"float\")).withColumn(\"longitude\", col(\"longitude\").cast(\"float\"))\n",
        "\n",
        "        # Remove any NULL filtering - keep all data\n",
        "        df = df.filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull() & col(\"analysis_neighborhood\").isNotNull())\n",
        "\n",
        "        print(f\"‚úÖ Loaded {df.count()} parking meters\")\n",
        "        return df\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"‚ùå Error loading data: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error in load_parking_meter_data: {e}\")\n",
        "        return None\n",
        "\n",
        "def validate_data(df):\n",
        "    \"\"\"Validate parking meter data for completeness.\"\"\"\n",
        "    if df is None:\n",
        "        print(\"‚ùå DataFrame is None\")\n",
        "        return False\n",
        "    required_columns = ['post_id', 'street_name', 'street_num', 'latitude', 'longitude', 'analysis_neighborhood']\n",
        "    missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"‚ùå Missing columns: {missing_cols}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def load_event_data():\n",
        "    \"\"\"Load realistic event data with moderate impacts on parking demand.\"\"\"\n",
        "    events = [\n",
        "        # Moderate impact events - more realistic than large spikes\n",
        "        {\"date\": \"2025-05-27\", \"hour\": 19, \"neighborhood\": \"Mission\", \"impact\": 0.08},        # Small concert\n",
        "        {\"date\": \"2025-05-28\", \"hour\": 20, \"neighborhood\": \"Downtown\", \"impact\": 0.12},       # Business event\n",
        "        {\"date\": \"2025-05-27\", \"hour\": 18, \"neighborhood\": \"SoMa\", \"impact\": 0.10},          # Tech meetup\n",
        "        {\"date\": \"2025-05-28\", \"hour\": 19, \"neighborhood\": \"Castro\", \"impact\": 0.06},        # Community event\n",
        "        {\"date\": \"2025-05-27\", \"hour\": 20, \"neighborhood\": \"Marina\", \"impact\": 0.08},        # Restaurant event\n",
        "        {\"date\": \"2025-05-28\", \"hour\": 18, \"neighborhood\": \"Nob Hill\", \"impact\": 0.10},      # Hotel conference\n",
        "\n",
        "        # Weekend events with slightly higher impact\n",
        "        {\"date\": \"2025-05-31\", \"hour\": 14, \"neighborhood\": \"Chinatown\", \"impact\": 0.15},     # Saturday festival\n",
        "        {\"date\": \"2025-05-31\", \"hour\": 20, \"neighborhood\": \"North Beach\", \"impact\": 0.12},   # Saturday nightlife\n",
        "        {\"date\": \"2025-06-01\", \"hour\": 13, \"neighborhood\": \"Marina\", \"impact\": 0.18},        # Sunday farmers market\n",
        "        {\"date\": \"2025-06-01\", \"hour\": 15, \"neighborhood\": \"Golden Gate Park\", \"impact\": 0.20}, # Sunday park event\n",
        "\n",
        "        # Negative impact events (reduced demand)\n",
        "        {\"date\": \"2025-05-29\", \"hour\": 8, \"neighborhood\": \"Financial District\", \"impact\": -0.15}, # Holiday - less business\n",
        "        {\"date\": \"2025-05-30\", \"hour\": 16, \"neighborhood\": \"SoMa\", \"impact\": -0.10},         # Early Friday closure\n",
        "    ]\n",
        "    return spark.createDataFrame(pd.DataFrame(events))\n",
        "\n",
        "def simulate_occupancy_batch(df, target_date, target_hour):\n",
        "    \"\"\"Simulate realistic occupancy with guaranteed availability across the city.\"\"\"\n",
        "    if df is None:\n",
        "        print(\"‚ùå Input DataFrame is None\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        from pyspark.sql.functions import when, rand, lit, dayofweek\n",
        "\n",
        "        # Create timestamp for all meters\n",
        "        target_timestamp = datetime.combine(target_date, datetime.min.time()) + timedelta(hours=target_hour)\n",
        "\n",
        "        # Add timestamp to ALL meters (no sampling)\n",
        "        full_df = df.withColumn(\"timestamp\", lit(target_timestamp))\n",
        "\n",
        "        # Join with event data\n",
        "        event_df = load_event_data()\n",
        "        full_df = full_df.join(event_df,\n",
        "                              (col(\"timestamp\").cast(\"date\") == col(\"date\")) &\n",
        "                              (col(\"analysis_neighborhood\") == col(\"neighborhood\")),\n",
        "                              \"left_outer\")\n",
        "\n",
        "        # FIXED: More realistic hourly occupancy patterns with guaranteed availability\n",
        "        realistic_hour_occupancy = {\n",
        "            # Late night/Early morning - lower occupancy, more availability\n",
        "            0: 0.15,   # 15% occupied, 85% available\n",
        "            1: 0.12,   # 12% occupied, 88% available\n",
        "            2: 0.10,   # 10% occupied, 90% available\n",
        "            3: 0.08,   # 8% occupied, 92% available\n",
        "            4: 0.10,   # 10% occupied, 90% available\n",
        "            5: 0.15,   # 15% occupied, 85% available\n",
        "\n",
        "            # Morning - gradual realistic increase\n",
        "            6: 0.25,   # 25% occupied, 75% available\n",
        "            7: 0.40,   # 40% occupied, 60% available\n",
        "            8: 0.55,   # 55% occupied, 45% available\n",
        "            9: 0.65,   # 65% occupied, 35% available\n",
        "\n",
        "            # Business hours - high but never complete occupancy\n",
        "            10: 0.70,  # 70% occupied, 30% available\n",
        "            11: 0.75,  # 75% occupied, 25% available\n",
        "            12: 0.78,  # 78% occupied, 22% available (peak)\n",
        "            13: 0.75,  # 75% occupied, 25% available\n",
        "            14: 0.72,  # 72% occupied, 28% available\n",
        "            15: 0.68,  # 68% occupied, 32% available\n",
        "\n",
        "            # Afternoon/Evening - gradual decrease\n",
        "            16: 0.60,  # 60% occupied, 40% available\n",
        "            17: 0.50,  # 50% occupied, 50% available (turnover)\n",
        "            18: 0.45,  # 45% occupied, 55% available\n",
        "            19: 0.40,  # 40% occupied, 60% available\n",
        "            20: 0.35,  # 35% occupied, 65% available\n",
        "            21: 0.30,  # 30% occupied, 70% available\n",
        "            22: 0.25,  # 25% occupied, 75% available\n",
        "            23: 0.20   # 20% occupied, 80% available\n",
        "        }\n",
        "\n",
        "        # Get base occupancy rate for the hour\n",
        "        base_occupancy = realistic_hour_occupancy.get(target_hour, 0.4)\n",
        "\n",
        "        # FIXED: More realistic neighborhood multipliers that don't cause 100% occupancy\n",
        "        neighborhood_multipliers = {\n",
        "            # High-demand areas - but not extreme\n",
        "            'Financial District/South Beach': 1.3,\n",
        "            'South of Market': 1.25,\n",
        "            'Downtown': 1.25,\n",
        "            'Mission Bay': 1.2,\n",
        "            'Tenderloin': 1.15,\n",
        "\n",
        "            # Medium-high demand\n",
        "            'Nob Hill': 1.1,\n",
        "            'North Beach': 1.1,\n",
        "            'Mission': 1.08,\n",
        "            'Castro/Upper Market': 1.05,\n",
        "            'Hayes Valley': 1.05,\n",
        "\n",
        "            # Medium demand\n",
        "            'Chinatown': 1.0,\n",
        "            'Marina': 1.0,\n",
        "            'Russian Hill': 1.0,\n",
        "            'Pacific Heights': 1.0,\n",
        "            'Potrero Hill': 0.95,\n",
        "\n",
        "            # Lower demand areas\n",
        "            'Inner Richmond': 0.85,\n",
        "            'Inner Sunset': 0.80,\n",
        "            'Outer Richmond': 0.75,\n",
        "            'Sunset/Parkside': 0.70,\n",
        "            'Bernal Heights': 0.75,\n",
        "            'Excelsior': 0.65,\n",
        "            'Visitacion Valley': 0.60,\n",
        "            'Bayview Hunters Point': 0.65,\n",
        "            'Outer Mission': 0.70\n",
        "        }\n",
        "\n",
        "        # Apply neighborhood-specific adjustments with fallback\n",
        "        neighborhood_expr = lit(0.9)  # default multiplier for unlisted neighborhoods\n",
        "        for neighborhood, multiplier in neighborhood_multipliers.items():\n",
        "            neighborhood_expr = when(col(\"analysis_neighborhood\") == neighborhood,\n",
        "                                   lit(multiplier)).otherwise(neighborhood_expr)\n",
        "\n",
        "        full_df = full_df.withColumn(\"neighborhood_multiplier\", neighborhood_expr)\n",
        "\n",
        "        # Calculate adjusted occupancy probability\n",
        "        full_df = full_df.withColumn(\"occupancy_prob\",\n",
        "                                   col(\"neighborhood_multiplier\") * lit(base_occupancy) +\n",
        "                                   when(col(\"impact\").isNotNull(), col(\"impact\")).otherwise(0.0))\n",
        "\n",
        "        # CRITICAL FIX: Cap probability to ensure availability ALWAYS exists\n",
        "        # Never allow more than 85% occupancy to guarantee spots are available\n",
        "        full_df = full_df.withColumn(\"occupancy_prob\",\n",
        "                                   when(col(\"occupancy_prob\") > 0.85, 0.85)  # Max 85% occupied = 15% available\n",
        "                                   .when(col(\"occupancy_prob\") < 0.05, 0.05)  # Min 5% occupied\n",
        "                                   .otherwise(col(\"occupancy_prob\")))\n",
        "\n",
        "        # FIXED: Day-of-week variations that are more moderate\n",
        "        is_weekend = target_timestamp.weekday() >= 5\n",
        "\n",
        "        if is_weekend:\n",
        "            # Weekend adjustments - more moderate\n",
        "            weekend_adjustments = {\n",
        "                6: 0.8,   # Less early morning activity\n",
        "                7: 0.85,  #\n",
        "                8: 0.9,   #\n",
        "                9: 0.95,  #\n",
        "                10: 1.05, # Weekend activity picks up later\n",
        "                11: 1.1,  #\n",
        "                12: 1.15, # Weekend peak but moderate\n",
        "                13: 1.1,  #\n",
        "                14: 1.05, #\n",
        "                15: 1.0,  #\n",
        "                16: 0.95, #\n",
        "                17: 0.9,  #\n",
        "                18: 1.05, # Weekend dinner\n",
        "                19: 1.1,  #\n",
        "                20: 1.05, #\n",
        "                21: 1.0   #\n",
        "            }\n",
        "            weekend_mult = weekend_adjustments.get(target_hour, 1.0)\n",
        "            full_df = full_df.withColumn(\"occupancy_prob\", col(\"occupancy_prob\") * lit(weekend_mult))\n",
        "\n",
        "            # RE-CAP after weekend adjustment\n",
        "            full_df = full_df.withColumn(\"occupancy_prob\",\n",
        "                                       when(col(\"occupancy_prob\") > 0.85, 0.85)\n",
        "                                       .otherwise(col(\"occupancy_prob\")))\n",
        "\n",
        "        # FIXED: Use multiple random seeds to create realistic distribution\n",
        "        # This prevents all meters from having similar occupancy patterns\n",
        "        full_df = full_df.withColumn(\"random_val\", rand())  # Remove fixed seed for more variation\n",
        "        full_df = full_df.withColumn(\"occupied\",\n",
        "                                   when(col(\"random_val\") < col(\"occupancy_prob\"), 1).otherwise(0))\n",
        "\n",
        "        # Add time-based columns\n",
        "        full_df = full_df.withColumn(\"hour\", lit(target_hour))\n",
        "        full_df = full_df.withColumn(\"dayofweek\", dayofweek(col(\"timestamp\")))\n",
        "\n",
        "        # Clean up intermediate columns\n",
        "        full_df = full_df.drop(\"date\", \"neighborhood\", \"impact\",\n",
        "                             \"neighborhood_multiplier\", \"occupancy_prob\", \"random_val\")\n",
        "\n",
        "        # Calculate and display statistics\n",
        "        total_count = full_df.count()\n",
        "        occupied_count = full_df.filter(col(\"occupied\") == 1).count()\n",
        "        available_count = total_count - occupied_count\n",
        "        occupancy_rate = occupied_count / total_count if total_count > 0 else 0\n",
        "\n",
        "        print(f\"‚úÖ Realistic occupancy simulation complete:\")\n",
        "        print(f\"   üìä Total meters: {total_count:,}\")\n",
        "        print(f\"   üöó Occupied: {occupied_count:,} ({occupancy_rate:.1%})\")\n",
        "        print(f\"   üÖøÔ∏è Available: {available_count:,} ({1-occupancy_rate:.1%})\")\n",
        "        print(f\"   ‚è∞ Time: {target_hour}:00 ({'Weekend' if is_weekend else 'Weekday'})\")\n",
        "\n",
        "        # VERIFICATION: Ensure we have availability in multiple neighborhoods\n",
        "        neighborhood_stats = full_df.groupBy(\"analysis_neighborhood\").agg(\n",
        "            count(\"*\").alias(\"total\"),\n",
        "            count(when(col(\"occupied\") == 0, 1)).alias(\"available\")\n",
        "        ).collect()\n",
        "\n",
        "        neighborhoods_with_spots = sum(1 for row in neighborhood_stats if row.available > 0)\n",
        "        print(f\"   üèòÔ∏è Neighborhoods with available spots: {neighborhoods_with_spots}\")\n",
        "\n",
        "        # Warning if too few neighborhoods have availability\n",
        "        if neighborhoods_with_spots < 5:\n",
        "            print(\"   ‚ö†Ô∏è Warning: Very few neighborhoods have availability - this may be unrealistic\")\n",
        "\n",
        "        return full_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in simulate_occupancy_batch: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def train_improved_predictor(df):\n",
        "    \"\"\"FIXED: Train model that properly learns occupancy patterns without extreme predictions.\"\"\"\n",
        "    try:\n",
        "        # Use ALL available data for training\n",
        "        pandas_df = df.select(\"hour\", \"dayofweek\", \"latitude\", \"longitude\", \"occupied\").toPandas()\n",
        "\n",
        "        features = ['hour', 'dayofweek', 'latitude', 'longitude']\n",
        "        X = pandas_df[features].astype(float)\n",
        "        y = pandas_df['occupied'].astype(int)\n",
        "\n",
        "        print(f\"üß† Training improved model: {len(X)} samples\")\n",
        "\n",
        "        # Check class distribution\n",
        "        occupied_rate = y.mean()\n",
        "        print(f\"üìä Training data: {occupied_rate:.1%} occupied, {1-occupied_rate:.1%} available\")\n",
        "\n",
        "        # Train-test split with stratification to maintain class balance\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # FIXED: Better model architecture with dropout to prevent overfitting\n",
        "        model = Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(4,)),\n",
        "            Dropout(0.3),  # Prevent overfitting\n",
        "            Dense(32, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1, activation='sigmoid')  # Output probability of being occupied\n",
        "        ])\n",
        "\n",
        "        # FIXED: Better compilation with class weights to handle imbalanced data\n",
        "        # Calculate class weights to handle imbalanced dataset\n",
        "        from sklearn.utils.class_weight import compute_class_weight\n",
        "        classes = np.unique(y_train)\n",
        "        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "        class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        # Train with early stopping and class weights\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=30,  # Reduced epochs to prevent overfitting\n",
        "            batch_size=256,\n",
        "            verbose=1,\n",
        "            class_weight=class_weight_dict,  # Handle class imbalance\n",
        "            callbacks=[EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True)]\n",
        "        )\n",
        "\n",
        "        # Evaluate with detailed metrics\n",
        "        _, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f\"üéØ Model Performance:\")\n",
        "        print(f\"   Accuracy: {accuracy:.3f}\")\n",
        "        print(f\"   Precision: {precision:.3f}\")\n",
        "        print(f\"   Recall: {recall:.3f}\")\n",
        "\n",
        "        # Test prediction distribution on validation set\n",
        "        val_predictions = model.predict(X_test, verbose=0)\n",
        "        val_pred_rate = (val_predictions > 0.5).mean()\n",
        "        print(f\"   Predicted occupancy rate: {val_pred_rate:.1%}\")\n",
        "\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in train_improved_predictor: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def predict_availability_fixed(model, df, user_time, neighborhood):\n",
        "    \"\"\"FIXED: Properly interpret model predictions and ensure realistic availability.\"\"\"\n",
        "    try:\n",
        "        # Get meters for the specific neighborhood and time\n",
        "        input_df = df.filter(\n",
        "            (col(\"timestamp\") == user_time) &\n",
        "            (col(\"analysis_neighborhood\") == neighborhood)\n",
        "        ).select(\"hour\", \"dayofweek\", \"latitude\", \"longitude\", \"street_name\", \"street_num\", \"post_id\", \"occupied\")\n",
        "\n",
        "        pandas_input = input_df.toPandas()\n",
        "        if pandas_input.empty:\n",
        "            print(f\"‚ùå No meters found for {neighborhood}\")\n",
        "            return None\n",
        "\n",
        "        # Get actual occupancy from simulation for comparison\n",
        "        actual_available = (pandas_input['occupied'] == 0).sum()\n",
        "        actual_occupied = (pandas_input['occupied'] == 1).sum()\n",
        "        actual_rate = actual_occupied / len(pandas_input)\n",
        "\n",
        "        print(f\"üìä {neighborhood} - Actual: {actual_occupied} occupied, {actual_available} available ({actual_rate:.1%} occupied)\")\n",
        "\n",
        "        # Make predictions using the ML model\n",
        "        X = pandas_input[['hour', 'dayofweek', 'latitude', 'longitude']].astype(float)\n",
        "        occupation_probabilities = model.predict(X, verbose=0).flatten()\n",
        "\n",
        "        # CRITICAL FIX: Use a dynamic threshold based on the expected occupancy rate\n",
        "        # Instead of a fixed 0.4 or 0.5 threshold, use the median probability\n",
        "        dynamic_threshold = np.percentile(occupation_probabilities, 50)  # 50th percentile\n",
        "\n",
        "        # Ensure threshold is reasonable (between 0.3 and 0.7)\n",
        "        dynamic_threshold = max(0.3, min(0.7, dynamic_threshold))\n",
        "\n",
        "        print(f\"üéØ Using dynamic threshold: {dynamic_threshold:.3f}\")\n",
        "        print(f\"üìà Probability range: {occupation_probabilities.min():.3f} to {occupation_probabilities.max():.3f}\")\n",
        "\n",
        "        # Predict availability (0 = available, 1 = occupied)\n",
        "        predicted_occupied = (occupation_probabilities >= dynamic_threshold).astype(int)\n",
        "        predicted_available = 1 - predicted_occupied\n",
        "\n",
        "        # Calculate confidence as distance from threshold\n",
        "        confidence_scores = np.abs(occupation_probabilities - dynamic_threshold)\n",
        "\n",
        "        # Create results dataframe\n",
        "        result_df = pandas_input.copy()\n",
        "        result_df['predicted_available'] = predicted_available\n",
        "        result_df['occupation_probability'] = occupation_probabilities\n",
        "        result_df['confidence'] = confidence_scores\n",
        "\n",
        "        # Filter for predicted available spots\n",
        "        available_spots = result_df[result_df['predicted_available'] == 1].copy()\n",
        "\n",
        "        # Sort by confidence (higher confidence first)\n",
        "        available_spots = available_spots.sort_values('confidence', ascending=False)\n",
        "\n",
        "        predicted_available_count = len(available_spots)\n",
        "        predicted_occupied_count = len(result_df) - predicted_available_count\n",
        "        predicted_rate = predicted_occupied_count / len(result_df)\n",
        "\n",
        "        print(f\"ü§ñ ML Prediction: {predicted_occupied_count} occupied, {predicted_available_count} available ({predicted_rate:.1%} occupied)\")\n",
        "\n",
        "        # Quality check: If we predict 0 available spots but simulation shows availability, adjust\n",
        "        if predicted_available_count == 0 and actual_available > 0:\n",
        "            print(\"üîß Adjusting: ML predicted 0 available but simulation shows availability\")\n",
        "            # Take the spots with lowest occupation probability as available\n",
        "            n_to_flip = min(5, actual_available)  # Take up to 5 spots or actual available, whichever is smaller\n",
        "            lowest_prob_indices = result_df.nsmallest(n_to_flip, 'occupation_probability').index\n",
        "\n",
        "            available_spots = result_df.loc[lowest_prob_indices].copy()\n",
        "            available_spots['predicted_available'] = 1\n",
        "            available_spots['confidence'] = 1 - available_spots['occupation_probability']\n",
        "\n",
        "            print(f\"üîß Adjusted to show {len(available_spots)} available spots\")\n",
        "\n",
        "        return available_spots if len(available_spots) > 0 else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in predict_availability_fixed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def recommend_spots(available_df, destination_lat, destination_lon, top_n=10):\n",
        "    \"\"\"Recommend top parking spots based on distance - increased default to 10.\"\"\"\n",
        "    if available_df is None or len(available_df) == 0:\n",
        "        print(\"‚ùå No available spots found\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Calculate distances for ALL available spots\n",
        "        available_df['distance'] = available_df.apply(\n",
        "            lambda row: geodesic((row['latitude'], row['longitude']),\n",
        "                               (destination_lat, destination_lon)).meters, axis=1)\n",
        "\n",
        "        # Get top recommendations from full dataset\n",
        "        top_spots = available_df.nsmallest(top_n, 'distance')\n",
        "\n",
        "        print(f\"‚úÖ Top {len(top_spots)} recommended spots from {len(available_df)} available:\")\n",
        "        for i, (_, row) in enumerate(top_spots.iterrows(), 1):\n",
        "            print(f\"  {i}. {row['street_num']} {row['street_name']} ({row['distance']:.0f}m away, confidence: {row['confidence']:.2f})\")\n",
        "\n",
        "        return top_spots\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in recommend_spots: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_comprehensive_map(spots_df, destination_lat, destination_lon, map_filename='parking_map_fixed_ml.html'):\n",
        "    \"\"\"Create comprehensive map with ALL available spots and destination marker.\"\"\"\n",
        "    if spots_df is None or len(spots_df) == 0:\n",
        "        print(\"‚ùå No spots to display on map\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Center on destination\n",
        "        center_lat = destination_lat\n",
        "        center_lon = destination_lon\n",
        "\n",
        "        m = folium.Map(location=[center_lat, center_lon], zoom_start=14)\n",
        "\n",
        "        # Add destination marker\n",
        "        folium.Marker(\n",
        "            location=[destination_lat, destination_lon],\n",
        "            popup=\"üéØ Your Destination\",\n",
        "            icon=folium.Icon(color='red', icon='star', prefix='fa')\n",
        "        ).add_to(m)\n",
        "\n",
        "        # Add markers for ALL available spots with color coding by distance\n",
        "        for i, (_, row) in enumerate(spots_df.iterrows()):\n",
        "            # Color code by distance\n",
        "            if row['distance'] <= 100:\n",
        "                color = 'green'\n",
        "            elif row['distance'] <= 300:\n",
        "                color = 'orange'\n",
        "            else:\n",
        "                color = 'blue'\n",
        "\n",
        "            folium.Marker(\n",
        "                location=[row['latitude'], row['longitude']],\n",
        "                popup=f\"#{i+1}: {row['street_num']} {row['street_name']}<br>Distance: {row['distance']:.0f}m<br>Confidence: {row['confidence']:.2f}<br>Prob: {row.get('occupation_probability', 'N/A'):.2f}\",\n",
        "                icon=folium.Icon(color=color, icon='car', prefix='fa')\n",
        "            ).add_to(m)\n",
        "\n",
        "        # Add a circle around destination to show walking distance\n",
        "        folium.Circle(\n",
        "            location=[destination_lat, destination_lon],\n",
        "            radius=200,  # 200m walking radius\n",
        "            popup=\"200m walking radius\",\n",
        "            color='red',\n",
        "            fill=True,\n",
        "            fillOpacity=0.1\n",
        "        ).add_to(m)\n",
        "\n",
        "        m.save(map_filename)\n",
        "\n",
        "        print(f\"‚úÖ Map saved to '{map_filename}' with {len(spots_df)} parking spots\")\n",
        "        files.download(map_filename)\n",
        "        display(FileLink(map_filename))\n",
        "\n",
        "        return m\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating map: {e}\")\n",
        "        return None\n",
        "\n",
        "def comprehensive_parking_search_ml_fixed(neighborhood, search_date, search_hour, dest_lat, dest_lon):\n",
        "    \"\"\"FIXED: Comprehensive parking search with properly working ML model.\"\"\"\n",
        "    print(f\"üîç ML-FIXED Search: {neighborhood} on {search_date} at {search_hour}:00\")\n",
        "    print(\"üìä Loading parking meter dataset...\")\n",
        "\n",
        "    # Load ALL data without limits\n",
        "    static_data = load_parking_meter_data()\n",
        "    if not validate_data(static_data):\n",
        "        return None\n",
        "\n",
        "    # Generate realistic occupancy for ALL meters\n",
        "    print(\"üéØ Generating realistic occupancy patterns...\")\n",
        "    occupancy_df = simulate_occupancy_batch(static_data, search_date, search_hour)\n",
        "    if occupancy_df is None:\n",
        "        return None\n",
        "\n",
        "    # Train improved ML model\n",
        "    print(\"üß† Training improved ML model...\")\n",
        "    model = train_improved_predictor(occupancy_df)\n",
        "    if model is None:\n",
        "        return None\n",
        "\n",
        "    # Use FIXED prediction method\n",
        "    user_time = datetime.combine(search_date, datetime.min.time()) + timedelta(hours=search_hour)\n",
        "    available_spots = predict_availability_fixed(model, occupancy_df, user_time, neighborhood)\n",
        "\n",
        "    # If no spots in target neighborhood, search nearby neighborhoods\n",
        "    if available_spots is None or len(available_spots) == 0:\n",
        "        print(f\"‚ùå No available spots in {neighborhood}\")\n",
        "        print(\"üîç Searching nearby neighborhoods...\")\n",
        "\n",
        "        # Get neighborhoods with most similar characteristics\n",
        "        all_neighborhoods = occupancy_df.select(\"analysis_neighborhood\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "        all_available = []\n",
        "\n",
        "        for hood in all_neighborhoods[:10]:  # Check first 10 neighborhoods\n",
        "            hood_spots = predict_availability_fixed(model, occupancy_df, user_time, hood)\n",
        "            if hood_spots is not None and len(hood_spots) > 0:\n",
        "                hood_spots['neighborhood'] = hood\n",
        "                all_available.append(hood_spots)\n",
        "\n",
        "        if all_available:\n",
        "            available_spots = pd.concat(all_available, ignore_index=True)\n",
        "            print(f\"‚úÖ Found {len(available_spots)} total available spots across multiple neighborhoods\")\n",
        "        else:\n",
        "            print(\"‚ùå No available spots found in searched neighborhoods\")\n",
        "            return None\n",
        "\n",
        "    if available_spots is not None and len(available_spots) > 0:\n",
        "        # Get comprehensive recommendations\n",
        "        recommendations = recommend_spots(available_spots, dest_lat, dest_lon, top_n=15)\n",
        "\n",
        "        # Create comprehensive map\n",
        "        if recommendations is not None:\n",
        "            map_filename = f'ml_fixed_parking_map_{neighborhood}_{search_date}_{search_hour}.html'\n",
        "            create_comprehensive_map(recommendations, dest_lat, dest_lon, map_filename)\n",
        "\n",
        "        return recommendations\n",
        "    else:\n",
        "        print(\"‚ùå No available spots found\")\n",
        "        return None\n",
        "\n",
        "def interactive_app_ml_fixed():\n",
        "    \"\"\"ML-FIXED interactive app with properly working machine learning model.\"\"\"\n",
        "    print(\"üìä Loading dataset to get all neighborhoods...\")\n",
        "\n",
        "    # Load full dataset to get all neighborhoods\n",
        "    full_data = load_parking_meter_data()\n",
        "    if full_data is None:\n",
        "        return\n",
        "\n",
        "    all_neighborhoods = sorted(full_data.select(\"analysis_neighborhood\").distinct().rdd.flatMap(lambda x: x).collect())\n",
        "    print(f\"‚úÖ Found {len(all_neighborhoods)} neighborhoods in dataset\")\n",
        "\n",
        "    neighborhood_dropdown = widgets.Dropdown(options=all_neighborhoods, description='Neighborhood:')\n",
        "    date_picker = widgets.DatePicker(description='Date:', value=date.today())\n",
        "    hour_slider = widgets.IntSlider(description='Hour:', value=10, min=0, max=23)\n",
        "    lat_input = widgets.FloatText(description='Dest. Latitude:', value=37.7749)\n",
        "    lon_input = widgets.FloatText(description='Dest. Longitude:', value=-122.4194)\n",
        "    search_button = widgets.Button(description='üîç Find Parking', button_style='success')\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_search_click(b):\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            try:\n",
        "                results = comprehensive_parking_search_ml_fixed(\n",
        "                    neighborhood_dropdown.value,\n",
        "                    date_picker.value,\n",
        "                    hour_slider.value,\n",
        "                    lat_input.value,\n",
        "                    lon_input.value\n",
        "                )\n",
        "                if results is not None:\n",
        "                    print(\"üéâ Search completed successfully!\")\n",
        "                else:\n",
        "                    print(\"‚ùå Search completed but no results found\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error during search: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "    search_button.on_click(on_search_click)\n",
        "\n",
        "    # Display the interface\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\"<h2>üÖøÔ∏è Smart Parking Finder - ML Enhanced</h2>\"),\n",
        "        neighborhood_dropdown,\n",
        "        date_picker,\n",
        "        hour_slider,\n",
        "        lat_input,\n",
        "        lon_input,\n",
        "        search_button,\n",
        "        output\n",
        "    ]))\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the complete parking finder system.\"\"\"\n",
        "    print(\"üÖøÔ∏è Smart Parking Finder with Fixed ML Implementation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Option 1: Run interactive app\n",
        "        print(\"Starting interactive app...\")\n",
        "        interactive_app_ml_fixed()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in main: {e}\")\n",
        "\n",
        "        # Option 2: Run a sample search if interactive fails\n",
        "        print(\"\\nüîß Running sample search instead...\")\n",
        "        sample_results = comprehensive_parking_search_ml_fixed(\n",
        "            neighborhood=\"Mission\",\n",
        "            search_date=date.today(),\n",
        "            search_hour=14,\n",
        "            dest_lat=37.7599,\n",
        "            dest_lon=-122.4148\n",
        "        )\n",
        "\n",
        "        if sample_results is not None:\n",
        "            print(\"‚úÖ Sample search completed successfully!\")\n",
        "        else:\n",
        "            print(\"‚ùå Sample search failed\")\n",
        "\n",
        "def cleanup_resources():\n",
        "    \"\"\"Clean up Spark resources.\"\"\"\n",
        "    try:\n",
        "        spark.stop()\n",
        "        gc.collect()\n",
        "        print(\"‚úÖ Resources cleaned up\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Ensure cleanup happens at end\n",
        "import atexit\n",
        "atexit.register(cleanup_resources)\n",
        "\n",
        "# Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Destination Latitude and Longitude Eaxmple:\n",
        "\n",
        "* California Academy of Sciences: 37.7699¬∞ N, 122.4661¬∞ W\n",
        "* UC Law San Francisco: 37.7811¬∞ N, 122.4158¬∞ W"
      ],
      "metadata": {
        "id": "-QA5Tq2J-fre"
      }
    }
  ]
}